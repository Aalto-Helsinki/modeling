A simple tool to model movements in a particle-like fashion
-----------------------------------------------
-----------------------------------------------
DEPENDENCIES:
-----------------------------------------------
This model needs python 3, matplotlib, numpy and scipy to function.
-----------------------------------------------
-----------------------------------------------
USAGE:
-----------------------------------------------
Currently this software is controlled through a settings file, where the user can define
the values needed for vis simulation.
Please do not edit vector2.py or objects.py, as these are needed for the correct function of
this model.
-----------------------------------------------
-----------------------------------------------
ALGORITHM:
-----------------------------------------------
1. Place enzymes and substrates in a container (circle with a radius)
2. With every timestep, move the enzymes and substrates.
Then, check if substrates are near enzymes. If they are, try to react them.
That has a probability of happening. If it happens, change the substrate to 
a different substrate.
3. repeat
4. After enough repetition, plot a curve from the amounts of the particles.
-----------------------------------------------
FEATURE REQUESTS:
-----------------------------------------------
This is a list of features that I (or you) want to get implemented in this
model. These features may or may not be implemented, but every one of them 
is checked for viability and relevance. If you have ideas about this model, then 
let's discuss things and add your suggestion!
Suggestions that have to do with pretty visualization/things "looking cool"
are currently put to the end of the list.
-----------------------------------------------

1. Busyness indicator for enzymes and substrates alike, so that the model can
account for enzymes and substrates being in an intermediate state.
This would also speed up the simulation, as there is no need to loop through enzymes/substrates that are already bonded.
|| DONE||

2. Is it better to iterate the substrates and enzymes as enzyme->sub or sub->enz?
If sub->enz, no substrate is bonded twice. If enz->sub, might be easier to
parallelize, but might also iterate more -> waste resources when compared
to the other situation.
||I'd say that the way it is now is better.||

3. A text-based UI/ settings file (to give input to the program without
having to edit the code). This is easy but takes a long time, but is also needed
for easy use of the model. Other thing that is perhaps more time-critical is a
batch-running script, so that we can have some data to decide if we should have
the micelles created.
||Settings file functionality created in server.py ||

4. Parallelization (create different threads for each enzyme?). This needs more work, as
currently I have no knowledge of how this can be implemented. One problem is the way
problem of multibonding: if 2 threads are bonding to the same substrate, how do
we settle this conflict? two substrates should NOT bond to same enzyme, that is
not smart. 
Perhaps map() could be used? it seems like a good candidate, along with the module multiprocessing.
||Multiprocessing was a mistake. The correct way of utilising a fast machine is to spawn multiple processes
of the simulation, thus utilising all of the processing power.||

5. Find out the connection between particle mass, temperature, dt, and delta
in brownian motion. currently I have no idea how these affect each other, only
that mass makes particles slower and temperature raises speed. Delta is the 
speed modifier, so how ideally we should define delta from mass and temperature.
||Mass is a simple divisor in place displacement per step (x = F*t^2/t). Temperature has to do with variance||

6. 3D space instead of 2D, since 2D may be too simplified. This is almost trivial,
and can be implemented in say, 3 hours. Big changes only include the vector class,
the way objects' places are updated.
||This does not seem relevant, perhaps some more research about the variance and stuff could be found. ||

7. A way to load the simulation parameters from a file and to save simulation data in a file.
This is relevant, since we can only get the data from a cluster in a file, I think.
||Done, this is default behaviour in server.py||


8. A graphical UI (for ease of access, though may be a bit hard to implement).
This should be one of the last things to implement, as it is not so relevant to
our modeling (we should make batch runs on aalto's cluster + make the model itself
better)
||This is not a priority. It will be tackled if/when I feel like it||

9. Other enzymes for the first reaction to indicate that the cell has other enzymes
consuming butyraldehyde, and scans to see if this changes the situation dramatically.
||DONE||
-------------------------------------------------
-------------------------------------------------
FQA/Frequently Questioned Answers:
-------------------------------------------------
Q:What is the "profiling_data"-folder? can i delete it?
A:profiling_data has information about the performance of the program.
This includes a specific list of the function calls the program makes, 
as well as the times this takes. Profiling data is important when people
are searching for bottlenecks in their code. To answer your question, no, 
it should not be deleted. Neither should anything else, unless you know that
it is obsolete.

Q:How do you use this thing? It has no input whatsoever and screams errors when I try to use it? 
A:The most recent version is the server.py file. You use it by giving it a settings file as an argument,
for example "python3 server.py example_settings" simulates according to file example_settings. See the example settings
file for more information about the settings, or ask Arto.

Q:This is slow.
A:Your computer is slow.
